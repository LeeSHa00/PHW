# -*- coding: utf-8 -*-
"""PHW1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XafaZs2xYfq8aoEUrTHRdPI9s9PvVrOo
"""

201931397 이승하
PHW 1

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
df = pd.read_csv("breast_cancer.csv")
df.head()
#check the null value
df.isnull().sum()
 
#data preprocessing
df =df.drop(labels=["Unnamed: 32"],axis=1)
df["diagnosis"] = df["diagnosis"].replace("M",1)
df["diagnosis"] = df["diagnosis"].replace("B",0)
#Check outliers
plt.figure(figsize = (20, 15))
plotnumber = 1

for column in df:
    if plotnumber <= 30:
        ax = plt.subplot(5, 6, plotnumber)
        sns.boxplot(x=data[column])
        plt.xlabel(column)
       
    plotnumber += 1
plt.title("Distribution")
plt.show()
#Heatmap
plt.figure(figsize = (30, 15))
sns.heatmap(data.corr(),annot=True)
 def calculate_vif(dataset):
    vif=pd.DataFrame()
    vif_features = dataset.columns
    vif["vif_values"] = [variance_inflation_factor(dataset.values,i) for i in range (dataset.shape[1])]
    return vif
features = data[['radius_mean', 'texture_mean', 
       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',
       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
       'fractal_dimension_se', 'radius_worst', 'texture_worst',
       'perimeter_worst', 'area_worst', 'smoothness_worst',
       'compactness_worst', 'concavity_worst', 'concave points_worst',
       'symmetry_worst', 'fractal_dimension_worst']]
calculate_vif(features)
  
#StandardScaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
#Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
classifier = LogisticRegression()
classifier.fit(X_train,Y_train)
y_pred = classifier.predict(X_test)
print(accuracy_score(Y_test, classifier.predict(X_test)))

>> 0.9941520467836257
print(confusion_matrix(Y_test, y_pred))
>> [[114   1]
   [  0  56]]
#Support Vector Machine
from sklearn.svm import SVC
svc_model = SVC()
svc_model.fit(X_train,Y_train)
>>SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False) 
y_pred_svc = svc_model.predict(X_test)
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print(accuracy_score(Y_test, y_pred_svc))
>>0.9824561403508771
#Decision Tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
def performance_metric(y_true, y_predict):
    #mse
    error = metrics.mean_squared_error(y_true, y_predict)
    return error

def fit_model(data, target):
    regressor = DecisionTreeRegressor()
    param_grid = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}
    scoring_fnc = metrics.make_scorer(performance_metric, False)
    reg = GridSearchCV(regressor, param_grid, scoring = scoring_fnc, cv = 3)
    reg.fit(data, target)
    return reg.best_estimator_

regression = fit_model(X_train, Y_train)
y_pred = classifier.predict(X_test)
print(accuracy_score(Y_test, classifier.predict(X_test)))
>> 0.9941520467836257


#MinMaxScaling
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
#Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
classifier = LogisticRegression()
classifier.fit(X_train,Y_train)
y_pred = classifier.predict(X_test)
print(accuracy_score(Y_test, classifier.predict(X_test)))
>> 0.9766081871345029
print(confusion_matrix(Y_test, y_pred))
>>[[113   2]
   [  2  54]]
#Support Vector Machine
from sklearn.svm import SVC
svc_model = SVC()
svc_model.fit(X_train,Y_train)
>> SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
y_pred_svc = svc_model.predict(X_test)
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print(accuracy_score(Y_test, y_pred_svc))
>> 0.9941520467836257
#Decision Tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
def performance_metric(y_true, y_predict):
    #mse
    error = metrics.mean_squared_error(y_true, y_predict)
    return error

def fit_model(data, target):
    regressor = DecisionTreeRegressor()
    param_grid = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}
    scoring_fnc = metrics.make_scorer(performance_metric, False)
    reg = GridSearchCV(regressor, param_grid, scoring = scoring_fnc, cv = 3)
    reg.fit(data, target)
    return reg.best_estimator_

regression = fit_model(X_train, Y_train)
y_pred = classifier.predict(X_test)
print(accuracy_score(Y_test, classifier.predict(X_test)))
>> 0.9766081871345029
#MaxAbsScaling
from sklearn.preprocessing import MaxAbsScaler
scaler = MaxAbsScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
#Logistic Regression
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
classifier = LogisticRegression()
classifier.fit(X_train,Y_train)
y_pred = classifier.predict(X_test)
print(accuracy_score(Y_test, classifier.predict(X_test)))
>> 0.9766081871345029
print(confusion_matrix(Y_test, y_pred))
>>[[113   2]
   [  2  54]]
#Support Vector Machine
from sklearn.svm import SVC
svc_model = SVC()
svc_model.fit(X_train,Y_train)
>> SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
y_pred_svc = svc_model.predict(X_test)
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print(accuracy_score(Y_test, y_pred_svc))
>> 0.9941520467836257
#Decision Tree
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
def performance_metric(y_true, y_predict):
    #mse
    error = metrics.mean_squared_error(y_true, y_predict)
    return error

def fit_model(data, target):
    regressor = DecisionTreeRegressor()
    param_grid = {'max_depth':(1,2,3,4,5,6,7,8,9,10)}
    scoring_fnc = metrics.make_scorer(performance_metric, False)
    reg = GridSearchCV(regressor, param_grid, scoring = scoring_fnc, cv = 3)
    reg.fit(data, target)
    return reg.best_estimator_

regression = fit_model(X_train, Y_train)
y_pred = classifier.predict(X_test)
print(accuracy_score(Y_test, classifier.predict(X_test)))
>> 0.9766081871345029

#Define ML Functions

def encoding(df, encoder):
    if encoder == 'OrdinalEncoding':
        enc = preprocessing.OrdinalEncoder()
        encoded = enc.fit_transform(df)
        encoded = pd.DataFrame(data=encoded, columns=df.columns)
    elif encoder == 'LabelEncoding':
        for i in df:
            df[i] = preprocessing.LabelEncoder().fit_transform(df[i])
        encoded = df
    elif encoder == 'OneHotEncoding':
        encoded = pd.get_dummies(data=df, columns=df.columns)

    return encoded

def switch(x):
    return {1: "StandardScaling + LabelEncoding",
            2: "MinMaxScaling + LabelEncoding",
            3: "MaxAbsScaling + LabelEncoding",
            4: "RobustScaling + LabelEncoding",
            5: "StandardScaling + OrdinalEncoding",
            6: "MinMaxScaling + OrdinalEncoding",
            7: "MaxAbsScaling + OrdinalEncoding",
            8: "RobustScaling + OrdinalEncoding",
            9: "StandardScaling + OneHotEncoding",
            10: "MinMaxScaling + OneHotEncoding",
            11: "MaxAbsScaling + OneHotEncoding",
            12: "RobustScaling + OneHotEncoding"}[x]

def scaling(df, scaler):
    if scaler == 'StandardScaling':
        scaler = preprocessing.MaxAbsScaler()
        scaled_df = scaler.fit_transform(df)
    elif scaler == 'MinMaxScaling':
        scaler = preprocessing.MinMaxScaler()
        scaled_df = scaler.fit_transform(df)
    elif scaler == 'MaxAbsScaling':
        scaler = preprocessing.RobustScaler()
        scaled_df = scaler.fit_transform(df)
    elif scaler == 'RobustScaling':
        scaler = preprocessing.StandardScaler()
        scaled_df = scaler.fit_transform(df)

    scaled_Df = pd.DataFrame(data=scaled_df, columns=df.columns)
    return scaled_Df

def machineLearning(df, target):
    
    X = df.drop(target, axis=1)
    y = df[target]

    # Divide X to Categorical X_cate and Numeric X_nume
    X_category = X.select_dtypes(include='object')
    X_numerical = X.select_dtypes(exclude='object')

    # Store each scaler and encoder in each list
    scaler_list = ['StandardScaling', 'MinMaxScaling', 'MaxAbsScaling', 'RobustScaling']
    encoder_list = ['OrdinalEncoding', 'LabelEncoding', 'OneHotEncoding']

    # Array to store encoded values
    dataEncodeList = ['Label', 'Ordinal', 'OneHot']

    # An array containing a combination of encoding and scaling
    dataFrameList = ['stl', 'mml', 'mal', 'rbl'
        , 'sto', 'mmo', 'mao', 'rbo'
        , 'stoh', 'mmoh', 'maoh', 'rboh']

    # The category column is encoded by calling the encoding function.
    for i in range(len(encoder_list)):
        df_category = encoding(X_category, encoder_list[i])
        dataEncodeList[i] = pd.concat([df_category, X_numerical], axis=1)

    # Scale the data that is Encoded
    for i in range(len(dataEncodeList)):
        for j in range(len(scaler_list)):
            if i == 0:
                dataFrameList[j] = scaling(dataEncodeList[i], scaler_list[j])
            elif i == 1:
                dataFrameList[i + j + 3] = scaling(dataEncodeList[i], scaler_list[j])
            else:
                dataFrameList[i + j + 6] = scaling(dataEncodeList[i], scaler_list[j])
    i = 1
    for x in dataFrameList:
        print("Combination {}: ".format(i) + switch(i))

 # Divide the data set into train and test
        X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)

machineLearning(df,df['diagnosis'])